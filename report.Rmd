---
title: "Quantified Self Movement Data Analysis"
author: "Thọ Duy Nguyễn"
date: "24 July 2015"
output: html_document
---

#Introduction
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement – a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it.

In this project, our goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways.

#Data processing
##Preparation
```{r warning=FALSE, message=FALSE}
library(caret)
library(randomForest)
library(rpart)
```
Also set up working directory and download data sets
```{r warning=FALSE, message=FALSE}
trainDataUrl <-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testDataUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
trainDataFile <- "./data/pml-training.csv"
testDataFile  <- "./data/pml-testing.csv"
if (!file.exists("./data")) {
    dir.create("./data")
}
if (!file.exists(trainDataFile)) {
    download.file(trainDataUrl, destfile=trainDataFile)
}
if (!file.exists(testDataFile)) {
    download.file(testDataUrl, destfile=testDataFile)
}
```
Load data from csv file
```{r warning=FALSE, message=FALSE}
trainDataRaw <- read.csv("./data/pml-training.csv", stringsAsFactors = FALSE,
                     header = TRUE, sep = ",", na.strings=c("NA","#DIV/0!",""))
testDataRaw <- read.csv("./data/pml-testing.csv", stringsAsFactors = FALSE,
                    na.strings=c("NA","#DIV/0!",""))
```
##Cleaning data
Basic descriptive information of our data: 
```{r}
dim(trainDataRaw)
dim(testDataRaw)
sum(complete.cases(trainDataRaw))
sum(complete.cases(testDataRaw))
```
Both train and test data are not clean. Below are the problems of the data:

- There are insignificant features (used to identify observers or not related to our analysis purposes) 
- Features (predictors) which all observers are  missing values. 
- Derivated predictors, summarized from other predictors but can't be used in analysis. These predictors start by `stddev_`, `min_`, `max_`, `kurtosis_`, `skewness_`, `var_`, `amplitude_`
- Predictors have one unique value (i.e. are zero variance) or predictors that are have both of the following characteristics: they have very few unique values relative to the number of samples and the ratio of the frequency of the most common value to the frequency of the second most common value is large (*Max Kuhn and Kjell Johnson, 2013*)

Cleaning data process must solve these problem in order to have a smaller and more significant predictors. Also, we must keep as many observers as possible. 

```{r cache=TRUE}
##Remove  features which all values is NA
trainDataRaw <- trainDataRaw[,colSums(is.na(trainDataRaw))<nrow(trainDataRaw)]
testDataRaw <- testDataRaw[,colSums(is.na(testDataRaw))<nrow(testDataRaw)]

##Remove insignificant predictors
miscPattern <- "^X|timestamp|window|user_name"
derivatedPattern <- "kurtosis_|skewness_|max_|min_|avg_|stddev_|var_|amplitude_"
removedPattern <- paste0(miscPattern, "|", derivatedPattern)
trainRemove <- grepl(removedPattern, names(trainDataRaw))
testRemove <- grepl(removedPattern, names(testDataRaw))

trainDataRaw <- trainDataRaw[,!trainRemove]
testDataRaw <- testDataRaw[,!testRemove]
trainDataRaw$classe <- as.factor(trainDataRaw$classe)

##Remove near zero and zero variance predictors
nearZeroAndZeroVarTrain <- nearZeroVar(trainDataRaw, saveMetrics = TRUE)
trainDataClean <- trainDataRaw[,nearZeroAndZeroVarTrain$zeroVar == FALSE &
                             nearZeroAndZeroVarTrain$nzv == FALSE]
nearZeroAndZeroVarTest <- nearZeroVar(testDataRaw, saveMetrics = TRUE)
testDataClean <- testDataRaw[,nearZeroAndZeroVarTest$zeroVar == FALSE &
                           nearZeroAndZeroVarTest$nzv == FALSE]

##Check if there is any missing values
sum(complete.cases(trainDataClean)) == nrow(trainDataClean)
sum(complete.cases(testDataClean)) == nrow(testDataClean)
dim(trainDataClean)
dim(testDataClean)

##Check if predictors of trainning and test is idencical
##Notice: train data use "classe" (last column of data set) 
##test data use "problem_id" (last column of data set) 
all.equal(names(trainDataClean[,-ncol(trainDataClean)]), 
          names(testDataClean[,-ncol(testDataClean)]))
```
The clean data have fewer predictors than the original one and we don't need to drop any observer in both train and test data.  

#Apply predictive model

##Slicing data
Our train data has `r nrow(trainDataClean)` observer with `r ncol(trainDataClean)-1` predictor to predict `classe`. The data is big enough for 2 subset: 

- 70% observers used for training the model.
- 30% observers used for validating the model.

```{r cache=TRUE}
##slice the data
set.seed(123456789)
inTrain <- createDataPartition(trainDataClean$classe, p=0.70, list=F)
trainData <- trainDataClean[inTrain,]
validationData <- trainDataClean[-inTrain,]
```

##Using Random Forest to predict 
Outcome variable (`classe`) is categorical. We need to use a classification model in order to predict the data. **CART** did not provide a good result (*please refer to Appendix for result of CART*). We use `randomForest` to build predictive modeling. This method has some benefit:

- is relatively insensitive to value of `mtry`
- pre-processing requirements are minimal
- measure of performance can be calculated. 

According to Max Kuhn and Kjell Johnson (2013), the number of fold cross validation usually are from 5 or 10 but there is no formal rule. When this value is larger, the difference in size between  the trainning set and the resampling subsets get smaller. That leads to the bias of the technique smaller. Therefore, The model is train with 10-fold cross validation. 

```{r cache=TRUE}
randomForestModel <- train(classe ~ ., data=trainData, method="rf",
                           trControl=trainControl(method="cv", 10))
randomForestModel
```
Using validation data set, we estimate performance of the model
```{r cache=TRUE}
validationPredict <- predict(randomForestModel, validationData)
confusionMatrix(validationData$classe, validationPredict)
```

```{r cache=TRUE}
accuracy <- postResample(validationPredict, validationData$classe)
accuracy
```


The out-of-sample error is `r 1 - as.numeric(confusionMatrix(validationData$classe, validationPredict)$overall[1])`.


##Predict test data
Model contructed by random forest method is quite good. Therefore, we applied it to test data to re-test it's performance

```{r cache=TRUE}
result <- predict(randomForestModel, testDataClean[, -length(names(testDataClean))])
result
##Generate file to submit 
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
pml_write_files(result)
```
The model passed all 20 tests. We could conclude that it's a good predictive model to use to predict the manner in which people did the exercise.

#Appendix
##Testing CART model
```{r cache=TRUE}
rpartModel <- train(classe ~ ., data=trainData, method="rpart")
rpartModel
```
The accuracy is lower than `randomForest` model.  

#References

- Max Kuhn, Kjell Johnson (2013). *Applied Predictive Modeling*. Springer 